{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f32e7b1",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1235d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ba10a",
   "metadata": {},
   "source": [
    "# Import and Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Dataset/Fake.csv\")\n",
    "df2 = pd.read_csv(\"Dataset/True.csv\")\n",
    "\n",
    "df1['label'] = 1\n",
    "df2['label'] = 0 \n",
    "\n",
    "df1 = df1.drop(columns = ['subject', 'date'])\n",
    "df2 = df2.drop(columns = ['subject', 'date'])\n",
    "\n",
    "df3 = pd.read_csv(\"Dataset/news.csv\")\n",
    "\n",
    "# Use replace to change 'FAKE' to 1 and 'REAL' to 0\n",
    "df3['label'] = df3['label'].replace({'FAKE': 1, 'REAL': 0})\n",
    "df3 = df3.drop(columns = df3.columns[0])\n",
    "\n",
    "df4 = pd.read_csv(\"Dataset/WELFake_Dataset.csv\")\n",
    "\n",
    "df4 = df4.drop(columns = df4.columns[0])\n",
    "df4['label'] = df4['label'].replace({1: 0, 0: 1})\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is the combined DataFrame\n",
    "true_count = df[df['label'] == 0].shape[0]\n",
    "false_count = df[df['label'] == 1].shape[0]\n",
    "\n",
    "# Create a pie chart\n",
    "labels = ['True', 'False']\n",
    "sizes = [true_count, false_count]\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "explode = (0.1, 0)  # explode the 1st slice (True)\n",
    "\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Distribution of True and False News')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5bc9e",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Rows with empty values\n",
    "print(df.shape[0])\n",
    "df = df.dropna()\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fdc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplciate data\n",
    "print(df.shape[0])\n",
    "df = df.drop_duplicates(subset=['title'], keep='first')\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7daa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle (randomize) the rows\n",
    "df = df.sample(frac=1, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title and label only\n",
    "df = df[['title','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e467ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing: remove punctuation by character\n",
    "df['title'] = df['title'] .apply(lambda x: x.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
    "\n",
    "#Remove Numbers\n",
    "df['title']= df['title'].str.replace(r'\\b\\d+\\b', '', regex=True)\n",
    "\n",
    "# Load English stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# Apply preprocessing: remove stopwords\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([word.lower() for word in x.split() if word.lower() not in stopwords_set]))\n",
    "\n",
    "# Apply preprocessing: convert text to lowercase\n",
    "df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "\n",
    "#Reduce words to root form\n",
    "stemmer = PorterStemmer()\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is the DataFrame containing 'title' and 'label'\n",
    "df['title_word_count'] = df['title'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(df['title_word_count'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Title Word Count')\n",
    "plt.xlabel('Number of Words in Title')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a208c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DataFrame\n",
    "df.to_pickle('Dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the DataFrame back\n",
    "df = pd.read_pickle('../Dataframe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff96c0",
   "metadata": {},
   "source": [
    "## BERT Model (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c08a99",
   "metadata": {},
   "source": [
    "### Call BERT Model and Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec216a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686221b",
   "metadata": {},
   "source": [
    "### Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd725d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Bert tokeniser\n",
    "max_length = 20\n",
    "\n",
    "tokenized_data = tokenizer.batch_encode_plus(df['title'].tolist(), max_length=max_length, pad_to_max_length=True, truncation=True, return_tensors='pt')\n",
    "labels = torch.tensor(df['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d21c5",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcf078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokenized_data.input_ids,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237356f",
   "metadata": {},
   "source": [
    "### DataLoader and Calling GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b572675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to PyTorch DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model and data to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794ba24",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755dbfeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop with tqdm for progress bar\n",
    "epochs = 4\n",
    "\n",
    "train_losses = []  # List to store training losses\n",
    "train_accuracies = []  # List to store training accuracies\n",
    "test_losses = []  # List to store test losses\n",
    "test_accuracies = []  # List to store test accuracies\n",
    "best_validation_loss = float('inf') \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # Create a tqdm progress bar for training\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    total_loss = 0.0  # Variable to store the total training loss\n",
    "    correct_predictions = 0  # Variable to store the number of correct predictions\n",
    "    \n",
    "    for batch_inputs, batch_labels in progress_bar:\n",
    "        # Move data to GPU\n",
    "        batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        total_loss += loss.item() * len(batch_labels)  # Accumulate the loss\n",
    "        \n",
    "        # Calculate number of correct predictions\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == batch_labels).item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'Loss': loss.item()})\n",
    "    \n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    average_train_loss = total_loss / len(train_dataloader.dataset)\n",
    "    train_accuracy = correct_predictions / len(train_dataloader.dataset)\n",
    "    print(f\"Training Loss: {average_train_loss}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    \n",
    "    train_losses.append(average_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_inputs)\n",
    "        test_logits = test_outputs.logits\n",
    "        test_loss = criterion(test_logits, test_labels).item()\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Calculate test accuracy\n",
    "        test_predictions = torch.argmax(test_logits, dim=1)\n",
    "        test_accuracy = accuracy_score(test_labels.cpu().numpy(), test_predictions.cpu().numpy())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(f\"Test Loss after Epoch {epoch + 1}: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # Check if the current model has the best validation loss\n",
    "    if test_loss < best_validation_loss:\n",
    "        best_validation_loss = test_loss\n",
    "        # Save the current best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b4270",
   "metadata": {},
   "source": [
    "### Evaluating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea9d1f",
   "metadata": {},
   "source": [
    "#### Basic Evaluaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "test_inputs_tensor = torch.tensor(test_inputs).to(device)\n",
    "test_labels_tensor = torch.tensor(test_labels).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(test_inputs_tensor)\n",
    "    test_logits = test_outputs.logits\n",
    "    test_predictions = torch.argmax(test_logits, dim=1)\n",
    "\n",
    "# Convert predictions and labels to numpy arrays\n",
    "test_predictions = test_predictions.cpu().numpy()\n",
    "test_labels = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "precision = precision_score(test_labels, test_predictions)\n",
    "recall = recall_score(test_labels, test_predictions)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(f\"Validation Precision: {precision}\")\n",
    "print(f\"Validation Recall: {recall}\")\n",
    "print(f\"Validation F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6e0ed",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cbar=True,\n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "\n",
    "plt.title('Base BERT Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744f3fa",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic (ROC) Curve and Area Under Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits_cpu = test_logits.cpu().numpy()\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, test_logits_cpu[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Base BERT - ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b38134",
   "metadata": {},
   "source": [
    "## BERT Model (Transfer Learning with layer freezing and custom final layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704fe51",
   "metadata": {},
   "source": [
    "### Freeze Layers and Add Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13307c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BERT layers\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Add final layers\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert, num_classes=2):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract [CLS] token from the last hidden state\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Assuming 'device' is already defined\n",
    "model = CustomBertModel(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa99ba9",
   "metadata": {},
   "source": [
    "### Tokenise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert tokeniser\n",
    "max_length = 20\n",
    "\n",
    "tokenized_data = tokenizer.batch_encode_plus(df['title'].tolist(), max_length=max_length, pad_to_max_length=True, truncation=True, return_tensors='pt')\n",
    "labels = torch.tensor(df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenized data\n",
    "torch.save(tokenized_data, 'tokenized_data.pt')\n",
    "\n",
    "# Save labels\n",
    "torch.save(labels, 'labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3b140",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ccb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokenized_data.input_ids,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfbfdc4",
   "metadata": {},
   "source": [
    "### DataLoader and Calling GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to PyTorch DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model and data to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0260dd",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41729aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with tqdm for progress bar\n",
    "epochs = 4\n",
    "\n",
    "train_losses = []  # List to store training losses\n",
    "train_accuracies = []  # List to store training accuracies\n",
    "test_losses = []  # List to store test losses\n",
    "test_accuracies = []  # List to store test accuracies\n",
    "best_validation_loss = float('inf')  # Initialize with a very large value\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Create a tqdm progress bar for training\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    total_loss = 0.0  # Variable to store the total training loss\n",
    "    correct_predictions = 0  # Variable to store the number of correct predictions\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_inputs, batch_labels in progress_bar:\n",
    "        # Move data to GPU\n",
    "        batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs, attention_mask=(batch_inputs != 0).float())\n",
    "        logits = outputs\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        total_loss += loss.item() * len(batch_labels)  # Accumulate the loss\n",
    "\n",
    "        # Calculate number of correct predictions\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == batch_labels).item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    average_train_loss = total_loss / len(train_dataloader.dataset)\n",
    "    train_accuracy = correct_predictions / len(train_dataloader.dataset)\n",
    "    print(f\"Training Loss: {average_train_loss}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "\n",
    "    train_losses.append(average_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Evaluate the model on the test set after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_inputs, attention_mask=(test_inputs != 0).float())\n",
    "        test_logits = test_outputs\n",
    "        test_loss = criterion(test_logits, test_labels).item()\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        test_predictions = torch.argmax(test_logits, dim=1)\n",
    "        test_accuracy = accuracy_score(test_labels.cpu().numpy(), test_predictions.cpu().numpy())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f\"Test Loss after Epoch {epoch + 1}: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Check if the current model has the best validation loss\n",
    "    if test_loss < best_validation_loss:\n",
    "        best_validation_loss = test_loss\n",
    "        # Save the current best model\n",
    "        torch.save(model.state_dict(), 'best_model_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model_1 = model\n",
    "best_model_1.load_state_dict(torch.load('best_model_1.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aed35e",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17fe7f",
   "metadata": {},
   "source": [
    "#### Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd4c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "best_model_1.eval()\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "test_inputs_tensor = torch.tensor(test_inputs).to(device)\n",
    "test_attention_mask_tensor = (test_inputs_tensor != 0).float().to(device)\n",
    "test_labels_tensor = torch.tensor(test_labels).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model_1(test_inputs_tensor, attention_mask=test_attention_mask_tensor)\n",
    "    test_logits = test_outputs\n",
    "\n",
    "# Perform necessary operations on logits\n",
    "test_predictions = torch.argmax(test_logits, dim=1)\n",
    "\n",
    "# Convert predictions and labels to numpy arrays\n",
    "test_predictions = test_predictions.cpu().numpy()\n",
    "test_labels = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "precision = precision_score(test_labels, test_predictions)\n",
    "recall = recall_score(test_labels, test_predictions)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(f\"Validation Precision: {precision}\")\n",
    "print(f\"Validation Recall: {recall}\")\n",
    "print(f\"Validation F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49523749",
   "metadata": {},
   "source": [
    "#### Confusion Martix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cbar=True,\n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "\n",
    "plt.title('Adapted BERT Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e0bb9",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic (ROC) Curve and Area Under Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the test_logits tensor to the CPU\n",
    "test_logits_cpu = test_logits.cpu().numpy()\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, test_logits_cpu[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Adapted BERT - ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911824d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
